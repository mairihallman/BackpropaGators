---
title: "Part 2 - Ablative Analysis 1"
author: "Mairi Hallman"
date: "2024-04-14"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_condaenv("4373-project",require=TRUE)
```

```{python}
import matplotlib.pyplot as plt
from tensorflow.config.experimental import enable_op_determinism
from tensorflow.keras.datasets import mnist
from tensorflow.keras.layers import Dense, Flatten, Input, MultiHeadAttention, Reshape, LayerNormalization
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.utils import set_random_seed
```
```{python}
enable_op_determinism()
set_random_seed(1)
```
```{python}
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

```
```{python}
n_val = int(len(x_train)*0.1)

x_val = x_train[:n_val]
y_val = y_train[:n_val]

x_train_nv = x_train[n_val:]
y_train_nv = y_train[n_val:]
```


```{python}
def my_model_attn_1(shape, target_shape, num_heads, key_dim, n, classes, learning_rate, x_train, y_train, epochs, batch_size, x_val, y_val):
    """
    Same as my_model_attn, but without layer normalization.
    
    Parameters:
    - shape: tuple, the shape of the input images ((28,28) for mnist)
    - target_shape: tuple, reshaping dimensions for multiheaded attentions
    - num_heads: integer, number of heads in the attention mechanism
    - key_dim: integer
    - n: int, the number of nodes in the hidden layer
    - classes: int, the number of classes (10 for mnist)
    - learning_rate: float, the learning rate
    - x_train: numpy.ndarray
    - y_train: numpy.ndarray
    - epochs: int
    - batch_size: int
    - x_val: numpy.ndarray
    - y_val: numpy.ndarray
      
    Returns:
    - The fitted model and the history object.
    """
    
    # model layers
    inputs = Input(shape=shape)
    x = Flatten()(inputs)
    x = Reshape(target_shape=target_shape)(x) # reshaping in preparation for multi-headed attention
    x = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x) # multi-headed attention layer; first x represents queries, second represents key/value pairs
    x = Flatten()(x)
    x = Dense(n, activation="tanh")(x)
    outputs = Dense(classes)(x)
    
    model = Model(inputs=inputs, outputs=outputs)

    # compile model
    model.compile(optimizer=SGD(learning_rate=learning_rate),
                  loss=SparseCategoricalCrossentropy(from_logits=True),
                  metrics=["accuracy"])
                  
    # history object (for plotting accuracy and loss)
    history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val))
    
    return model, history
    
```
```{python}
model_attn_1, history_attn_1 = my_model_attn_1(shape=(28,28),target_shape=(56,14),num_heads=2,key_dim=14,n=300,classes=10,learning_rate=0.01,x_train=x_train_nv,y_train =y_train_nv,epochs=30,batch_size=50,x_val=x_val,y_val=y_val)
```

```{python}
acc_attn_1 = history_attn_1.history['accuracy']
val_acc_attn_1 = history_attn_1.history['val_accuracy']
loss_attn_1 = history_attn_1.history['loss']
val_loss_attn_1 = history_attn_1.history['val_loss']
epochs_range_attn_1 = range(1, len(acc_attn_1) + 1)

plt.figure(figsize=(14, 5))

# plot training and validation accuracy per epoch
plt.subplot(1, 2, 1)
plt.plot(epochs_range_attn_1, acc_attn_1, label='Training Accuracy')
plt.plot(epochs_range_attn_1, val_acc_attn_1, label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()

# plot training and validation loss per epoch
plt.subplot(1, 2, 2)
plt.plot(epochs_range_attn_1, loss_attn_1, label='Training Loss')
plt.plot(epochs_range_attn_1, val_loss_attn_1, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Cross-Entropy Loss')
plt.title('Training and Validation Loss')
plt.legend()

plt.show()
```