---
title: "Part 2 - Attention Mechanism Implementation"
author: "Mairi Hallman"
date: "2024-04-14"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_condaenv("4373-project",require=TRUE)
```

```{python}
import matplotlib.pyplot as plt
from tensorflow.config.experimental import enable_op_determinism
from tensorflow.keras.datasets import mnist
from tensorflow.keras.layers import Dense, Flatten, Input, MultiHeadAttention, Reshape, LayerNormalization
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.utils import set_random_seed
```
```{python}
enable_op_determinism()
set_random_seed(1)
```
```{python}
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

```
```{python}
n_val = int(len(x_train)*0.1)

x_val = x_train[:n_val]
y_val = y_train[:n_val]

x_train_nv = x_train[n_val:]
y_train_nv = y_train[n_val:]
```


```{python}
def my_model_attn(shape, n, classes, learning_rate, x_train, y_train, epochs, batch_size, x_val, y_val):
    """
    Initializes, compiles, and fits a model with an attention mechanism layer.
    
    Parameters:
    - shape: tuple, the shape of the input images ((28,28) for mnist)
    - n: int, the number of nodes in the hidden layer
    - classes: int, the number of classes (10 for mnist)
    - learning_rate: float, the learning rate
    - x_train: numpy.ndarray
    - y_train: numpy.ndarray
    - epochs: int
    - batch_size: int
    - x_val: numpy.ndarray
    - y_val: numpy.ndarray
      
    Returns:
    - The fitted model and the history object.
    """
    
    # Model layers
    inputs = Input(shape=shape)
    x = Flatten()(inputs)
    x = LayerNormalization()(x) # layer normalization
    x = Reshape(shape)(x) # reshaping in preparation for multi-headed attention
    x = MultiHeadAttention(num_heads=2, key_dim=14)(x, x) # multi-headed attention layer
    x = Flatten()(x)
    x = Dense(n, activation="tanh")(x)
    outputs = Dense(classes)(x)
    
    model = Model(inputs=inputs, outputs=outputs)

    # Compile model
    model.compile(optimizer=SGD(learning_rate=learning_rate),
                  loss=SparseCategoricalCrossentropy(from_logits=True),
                  metrics=["accuracy"])
                  
    # History object (for plotting accuracy and loss)
    history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val))
    
    return model, history
    
```
```{python}
model, history = my_model(shape=(28,28),n=300,classes=10,learning_rate=0.01,x_train=x_train_nv,y_train =y_train_nv,epochs=25,batch_size=50,x_val=x_val,y_val=y_val)
```
```{python}
test_loss, test_acc = model.evaluate(x_test, y_test)
```

```{python}
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs_range = range(1, len(acc) + 1)

plt.figure(figsize=(14, 5))

# Plot training and validation accuracy per epoch
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()

# Plot training and validation loss per epoch
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Cross-Entropy Loss')
plt.title('Training and Validation Loss')
plt.legend()

plt.show()
```