---
title: "Final Project - MAT4373"
author: "Tanner Giddings"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{python}
#!pip install matplotlib
#!pip install pandas
#!pip install tqdm
```


**Question 1**

```{python}
import matplotlib.pyplot as plt
from scipy.io import loadmat
import pandas as pd

data = loadmat('mnist_all.mat')
data.keys()
```

```{python}
fig, ax = plt.subplots(2, 5)
for i in range(10):
  ax[i % 2][i % 5].imshow(data[f"train{i}"][0].reshape((28,28)), cmap='gray')
  ax[i % 2][i % 5].set_title(f"Number {i}")
plt.show()
```

```{python}
#Cleaning the data
#Taking too long to run
"""
import pandas as pd

def clean(D):
  df = []
  for i in range(0,10):
    dict = {"pixel" + str(j) : [D[f"train{i}"][k][j] for k in range(len(D[f"train{i}"]))] for j in range(28*28)}
    dict['Y'] = [i] * len(dict['pixel0'])
    df.append(pd.DataFrame(dict))
  return pd.concat(df)

data_clean = clean(data)
data_clean
"""
```

```{python}
from tqdm import tqdm
def clean(D):
  d = {}
  for i in range(0, 10):
    d["train" + str(i)] = [elem / 255 for elem in D["train" + str(i)] for i in range(0,10)]
  return d
data_clean = clean(data)

```


**Question 2**

```{python}
import numpy as np

def softmax(x):
  e_x = np.exp(x)
  return e_x / e_x.sum()

def ReLU(x):
  return [max(0, elem) for elem in x]

def forward(X, w, b):
  return np.matmul(X, w) + b

def predict(X, w, b):
  return softmax(forward(X, w, b))

w1 = np.random.rand(28*28, 9)
b1 = np.random.rand(9)

print(predict(np.reshape(data_clean['train0'][0], (28*28)), w1, b1), "\nThis is the predicted outputs for an untrained model")
```

**Part 3**

Let $L(\overrightarrow{y}, \overrightarrow{p}) = -\sum_{i=0}^9(y_i log(p_i) + (1-y_i)log(1-p_i))$ be the loss function at the end of the network for one sample, the variable $i$ here represents each class, and $y_i$ represents the one-hot encoded value of the class, e.g. $y_i = \begin{pmatrix}0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0\end{pmatrix}$ represents 2. Also, $p_i$ is the output of the softmax function.

\begin{align*}
\frac{\delta}{\delta p_i} L(\overrightarrow{y}, \overrightarrow{p}) & = \frac{\delta}{\delta p_i} (-\sum_{i=0}^9(y_i log(p_i) + (1-y_i)log(1-p_i)))\\
& = -\sum_{i=0}^9(\frac{y_i}{p_i} - \frac{1-y_i}{1-p_i})\\
& = -\sum_{i=0}^9(\frac{y_i - y_ip_i - p_i + y_ip_i}{p_i(1-p_i)})\\
& = -\sum_{i=0}^9\frac{y_i - p_i}{p_i (1-p_i)}\\
& \text{Suppose the class of the sample is $k$, so $y_k = 1$ and $y_i = 0 \text{ }\forall i\neq k$}\\
& = \sum_{i = 0, i \neq k}^9 \frac{1}{1-p_i} - \frac{1}{p_k}
\end{align*}

Since $p_i$ is the output of the softmax function, $p_i = \frac{e^{z_i}}{\sum_{j=0}^9e^{zj}}$. Here we find the derivative of $p_i$ in relation to $\overrightarrow{z} = \sigma (w \overrightarrow{x}_i + \overrightarrow{b})$, with $x_i \in \mathbb{R}^{n}$, $w \in \mathbb{R}^{9 \times n}$, $b \in \mathbb{R}^9$ and $\sigma(x) = \text{max}\{0, x\}$:

\begin{align*}
\frac{\delta}{\delta z_i}p_i & = \frac{\delta}{\delta z_i}\frac{e^{z_i}}{\sum_{j=0}^9e^{zj}}\\
& = \frac{e^{z_i} \sum_{j=0}^9e^{zj} - (e^{z_i})^2}{(\sum_{j=0}^9e^{zj})^2}\\
& = \frac{e^{z_i}}{\sum_{j=0}^9e^{zj}} - ()^2
\end{align*}

```{python}
#This is the implementation of the gradient of the cost function for b (db) and w (dw)
def backprop(y, p, xi):
  db = y-p
  dw = np.matmul(db, xi.T)
  return db, dw

def update_parameters(w, b, db, dw, alpha):
  w = w - alpha * dw
  b = b - alpha * db
  return w, b
  
```

