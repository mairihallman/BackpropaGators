---
fontsize: 12pt
geometry: "driver=luatex,paper=letterpaper,hmargin=2.0cm,vmargin=1.5cm,headsep=0.75cm,headheight=0.75cm,includeheadfoot"
output: 
  pdf_document: 
    includes:
      in_header: header.tex
    latex_engine: lualatex
    keep_tex: true
    extra_dependencies: ['enumitem', 'amsmath', 'amssymb']
---
\begin{titlepage}
        \centering
        \mbox{}
        \vfill
        {\Huge Handwritten Digit Recognition\\
        with Neural Networks\par}
        \vspace{2\baselineskip}
        {\large Tanner Giddings\\
        Mairi Hallman\\
        Victor Matyiku\\
        Mitchell Whalen\par}
        \vspace{4\baselineskip}
        {Statistical Machine Learning\\
        Winter 2024\par}
        \vspace{\baselineskip}
        {Universit\symbol{"00E9} d'Ottawa\\
        University of Ottawa\par}
        \vspace{2\baselineskip}
        \today
\end{titlepage}
```{r 'import-packages', echo = FALSE}
library(tinytex)
library(reticulate)
```
```{r 'chunk-settings', echo = FALSE}
#knitr::opts_chunk$set()
```
```{r 'set-python-seed'}
tensorflow::set_random_seed(seed = 1)
```
```{python}
import numpy
import tensorflow.nn
from tensorflow.keras.datasets import mnist
from tensorflow.keras.layers import Dense, Flatten, Input
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.models import Sequential

# load dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = numpy.divide(x_train, 255.0)
x_test = numpy.divide(x_test, 255.0)

# lifted from Mairi 1-2
def my_model(shape, n, x_train, y_train):
    """
    Initializes, compiles, and fits a model.
    
    Parameters:
    - shape: tuple, the shape of the input images ((28,28) for minst)
    - n: int, the number of classes (10 for minst)
    - x_train: numpy.ndarray
    - y_train: numpy.ndarray
    
    Returns:
    - The fitted model.
    """
    
    # initialize model
    model = Sequential([
        Input(shape=(28, 28)),
        Flatten(),
        Dense(10, activation="linear")
    ])

    # compile model
    model.compile(
      optimizer="adam",
      loss=SparseCategoricalCrossentropy(from_logits=True), # from_logits=True applies softmax to loss
      metrics=["accuracy"]
    )

    # fit model
    model.fit(x_train, y_train)
    
    return model

model = my_model((28,28),10,x_train,y_train)

# my code
from random import sample

chosen = sample(range(len(y_train)), k = 500)

probs = tensorflow.nn.softmax(model(x_train[chosen]))
correct_probs = numpy.zeros(shape = len(y_train[chosen]))
for i in range(len(y_train[chosen])):
  correct_probs[i] = probs[i][y_train[chosen][i]]
cost = -sum(numpy.log(correct_probs))

# provided code snippets
def softmax(y):
    '''Return the output of the softmax function for the matrix of output y. y
    is an NxM matrix where N is the number of outputs for a single case, and M
    is the number of cases'''
    return numpy.exp(y)/numpy.tile(sum(numpy.exp(y),0), (len(y),1))

def cost(y, y_):
    return -sum(y_*log(y))
def deriv_multilayer(W0, b0, W1, b1, x, L0, L1, y, y_):
    '''Incomplete function for computing the gradient of the cross-entropy
    cost function w.r.t the parameters of a neural network'''
    dCdL1 =  y - y_
    #dCdW1 =  dot(L0, dCdL1.T ) if you don't want the nonlinearity at the top layer
    dCdW1 =  dot(L0, ((1- L1**2)*dCdL1).T )

```